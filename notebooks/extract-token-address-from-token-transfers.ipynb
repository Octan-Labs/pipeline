{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DecimalType, DoubleType\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"fs.s3a.access.key\", \"Q3AM3UQ867SPQQA43P2F\") \\\n",
    "    .config(\"fs.s3a.secret.key\", \"zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\") \\\n",
    "    .config(\"fs.s3a.endpoint\", \"play.min.io:9000\") \\\n",
    "    .appName(\"VolumeCalculation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "token_transfers_schema = StructType([ \\\n",
    "    StructField(\"token_address\", StringType(), True), \\\n",
    "    StructField(\"from_address\", StringType(), True), \\\n",
    "    StructField(\"to_address\", StringType(), True), \\\n",
    "    StructField(\"value\", DecimalType(38, 0), True), \\\n",
    "    StructField(\"transaction_hash\", StringType(), True), \\\n",
    "    StructField(\"log_index\", LongType(), True), \\\n",
    "    StructField(\"block_number\", LongType(), True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokens_schema = StructType([ \\\n",
    "    StructField(\"address\", StringType(), True), \\\n",
    "    StructField(\"symbol\", StringType(), True), \\\n",
    "    StructField(\"name\", StringType(), True), \\\n",
    "    StructField(\"decimals\", LongType(), True), \\\n",
    "    StructField(\"total_supply\", LongType(), True), \\\n",
    "    StructField(\"block_number\", LongType(), True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "basePath = \"s3a://bsc-test-tx-volume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "token_transfers_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(token_transfers_schema) \\\n",
    "    .load(basePath + \"/token_transfers/*/*/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokens_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(tokens_schema) \\\n",
    "    .load(basePath + \"/tokens/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "token_transfers_df.createOrReplaceTempView(\"token_transfers_df\")\n",
    "tokens_df.createOrReplaceTempView(\"tokens_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokens_result_df = spark.sql(\"\"\"\n",
    "SELECT DISTINCT(tt.token_address)\n",
    "FROM token_transfers_df tt\n",
    "LEFT JOIN tokens_df t ON LOWER(tt.token_address) = LOWER(t.address)\n",
    "WHERE t.address IS NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "result_df.show(10, False) if result_df.count() > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "result_df.repartition(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\",True) \\\n",
    "    .csv(base_path + \"/token_transfer_addresses\" + str(math.floor(start_time)))\n",
    "\n",
    "time.time() - start_time"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
