{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c031afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse, sys\n",
    "\n",
    "# parser=argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument(\"-b\", \"--base_path\", help=\"S3 bucket base path\", required=True)\n",
    "# parser.add_argument(\"-s\", \"--start\", help=\"Start date to calculate\", required=True)\n",
    "# parser.add_argument(\"-e\", \"--end\", help=\"End date to calculate\", required=True)\n",
    "# parser.add_argument(\"-n\", \"--name\", help=\"Name of chain to calculate [bsc|eth|polygon]\", required=True)\n",
    "# parser.add_argument(\"-s\", \"--symbol\", help=\"Symbol of chain to calculate [BNB|ETH|MATIC]\", required=True)\n",
    "\n",
    "# args=parser.parse_args()\n",
    "\n",
    "# base_path = args.base_path\n",
    "# start = args.start\n",
    "# end = args.end\n",
    "# name = args.name # [bsc|eth|polygon]\n",
    "# symbol = args.symbol  # [BNB|ETH|MATIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36addc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \".\"\n",
    "start = \"2020-10-31\"\n",
    "end = \"2020-10-31\"\n",
    "name = \"bsc\" # [bsc|eth|polygon]\n",
    "symbol = \"BNB\" # [BNB|ETH|MATIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42fdf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_dates_in_range(start, end):\n",
    "    # Convert start and end strings to datetime objects\n",
    "    start_date = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "\n",
    "    # Create a list to store the dates within the range\n",
    "    dates_list = []\n",
    "\n",
    "    # Loop through the dates and append them to the list\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        dates_list.append(current_date.strftime(\"%Y-%m-%d\"))\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return dates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b453980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = get_dates_in_range(start, end)\n",
    "\n",
    "num_of_date = len(dates)\n",
    "time_range = \"\"\n",
    "\n",
    "if(num_of_date == 1):\n",
    "    time_range = \"date\"\n",
    "elif(num_of_date < 7 or num_of_date == 7):\n",
    "    time_range = \"week\"\n",
    "else:\n",
    "    time_range = \"month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61b28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46c44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DecimalType, DoubleType\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253ee044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/17 10:48:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PreTxAndVolumeCalculator\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990cdacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a19283",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_schema = StructType([ \\\n",
    "    StructField(\"address\", StringType(), True), \\\n",
    "    StructField(\"symbol\", StringType(), True), \\\n",
    "    StructField(\"name\", StringType(), True), \\\n",
    "    StructField(\"decimals\", LongType(), True), \\\n",
    "    StructField(\"total_supply\", DecimalType(38,0), True), \\\n",
    "    StructField(\"block_number\", LongType(), True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e65627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc_historical_schema = StructType([ \\\n",
    "    StructField(\"id\", LongType(), True), \\\n",
    "    StructField(\"rank\", LongType(), True), \\\n",
    "    StructField(\"name\", StringType(), True), \\\n",
    "    StructField(\"symbol\", StringType(), True), \\\n",
    "    StructField(\"open\", DoubleType(), True), \\\n",
    "    StructField(\"high\", DoubleType(), True), \\\n",
    "    StructField(\"low\", DoubleType(), True), \\\n",
    "    StructField(\"close\", DoubleType(), True), \\\n",
    "    StructField(\"volume\", DoubleType(), True), \\\n",
    "    StructField(\"marketCap\", DoubleType(), True), \\\n",
    "    StructField(\"timestamp\", LongType(), True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b173992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc_address_schema = StructType([ \\\n",
    "    StructField(\"rank\", LongType(), True), \\\n",
    "    StructField(\"bsc\", StringType(), True), \\\n",
    "    StructField(\"eth\", StringType(), True), \\\n",
    "    StructField(\"polygon\", StringType(), True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39c5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57442cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transfers_df = spark.read.format(\"parquet\") \\\n",
    "    .load(list(map(lambda date: \"{base_path}/token_transfers/date={date}/*.parquet\".format(base_path = base_path, date = date), dates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9d94c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read.format(\"parquet\") \\\n",
    "    .load(list(map(lambda date: \"{base_path}/transactions/date={date}/*.parquet\".format(base_path = base_path, date = date), dates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8e6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(tokens_schema) \\\n",
    "    .load(\"{base_path}/cmc_tokens/cmc_{name}_tokens.csv\".format(base_path = base_path, name = name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc6f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc_historicals_df = spark.read.format(\"csv\") \\\n",
    "    .schema(cmc_historical_schema) \\\n",
    "    .load(\"./cmc_historicals/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1daf2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc_addresses_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(cmc_address_schema) \\\n",
    "    .load(\"./cmc_addresses/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "581ea759",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc_addresses_df = cmc_addresses_df.dropDuplicates([\"{name}\".format(name = name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d99517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33d2e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44c8925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = transactions_df.drop(col(\"input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73eadf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d454e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transfers_df.createOrReplaceTempView(\"token_transfers\")\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "tokens_df.createOrReplaceTempView(\"tokens\")\n",
    "cmc_historicals_df.createOrReplaceTempView(\"cmc_historicals\")\n",
    "cmc_addresses_df.createOrReplaceTempView(\"cmc_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "320c0021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11391353607177734"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change name, symbol foreach networks\n",
    "\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "pre_tx_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    tx.block_number,\n",
    "    tx.from_address,\n",
    "    tx.to_address,\n",
    "    tx.gas,\n",
    "    tx.receipt_gas_used as gas_used,\n",
    "    tx.gas_price,\n",
    "    tx.value,\n",
    "    null as token_transfer,\n",
    "    null as token_contract,\n",
    "    ((tx.value / POWER(10, 18)) * cmc_h.open) AS volume,\n",
    "    (tx.receipt_gas_used * tx.gas_price) / POWER(10,18) as gas_spent,\n",
    "    ((tx.receipt_gas_used * tx.gas_price) / POWER(10,18)) * cmc_h.open as gas_spent_usd\n",
    "FROM transactions tx\n",
    "CROSS JOIN cmc_addresses cmc_addr\n",
    "LEFT JOIN cmc_historicals cmc_h \n",
    "    ON (\n",
    "         cmc_addr.rank = cmc_h.rank AND\n",
    "         tx.block_timestamp < timestamp(cmc_h.timestamp) AND \n",
    "         tx.block_timestamp >  timestamp(cmc_h.timestamp - 86400)\n",
    "    )\n",
    "WHERE cmc_h.symbol = '{symbol}'\n",
    "UNION ALL\n",
    "SELECT \n",
    "    tt.block_number,\n",
    "    tt.from_address,\n",
    "    tt.to_address,\n",
    "    tx.gas,\n",
    "    tx.receipt_gas_used as gas_used,\n",
    "    tx.gas_price,\n",
    "    null as value,\n",
    "    tt.value as token_transfer,\n",
    "    tt.token_address,\n",
    "    ((tt.value / POWER(10, t.decimals)) * cmc_h.open) AS volume,\n",
    "    (tx.receipt_gas_used * tx.gas_price) / POWER(10,18) as gas_spent,\n",
    "    ((tx.receipt_gas_used * tx.gas_price) / POWER(10,18)) * native_cmc_h.open as gas_spent_usd\n",
    "FROM token_transfers tt\n",
    "LEFT JOIN transactions tx ON tt.transaction_hash = tx.hash\n",
    "LEFT JOIN tokens t ON LOWER(tt.token_address) = LOWER(t.address)\n",
    "LEFT JOIN cmc_addresses cmc_addr \n",
    "    ON tt.token_address = cmc_addr.{name}\n",
    "LEFT JOIN cmc_historicals cmc_h \n",
    "    ON (\n",
    "        cmc_addr.rank = cmc_h.rank AND \n",
    "        tx.block_timestamp < timestamp(cmc_h.timestamp) AND \n",
    "        tx.block_timestamp >  timestamp(cmc_h.timestamp - 86400)\n",
    "    )\n",
    "LEFT JOIN cmc_historicals native_cmc_h \n",
    "    ON (\n",
    "         native_cmc_h.symbol = '{symbol}' AND \n",
    "         tx.block_timestamp < timestamp(native_cmc_h.timestamp) AND \n",
    "         tx.block_timestamp >  timestamp(native_cmc_h.timestamp - 86400)\n",
    "    )\n",
    "\"\"\".format(name = name, symbol = symbol)) \\\n",
    "    .withColumn('volume', format_number('volume', 10)) \\\n",
    "    .withColumn('gas_spent', format_number('gas_spent', 10)) \\\n",
    "    .withColumn('gas_spent_usd', format_number('gas_spent_usd', 10))\n",
    "\n",
    "\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3b6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "564d4223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.278391122817993"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pre_tx_df.repartition(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\",True) \\\n",
    "    .csv(\n",
    "        \"{base_path}/pre_tx/{time_range}/start={start}_end={end}/\" \\\n",
    "            .format(\n",
    "                base_path = base_path, \\\n",
    "                time_range = time_range, \\\n",
    "                start = start, \\\n",
    "                end = end \\\n",
    "            ) \\\n",
    "    )\n",
    "\n",
    "\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd6141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c374c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tx_df.createOrReplaceTempView(\"pre_tx_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203a0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "223e6a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09346485137939453"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change name, symbol foreach networks\n",
    "\n",
    "import time\n",
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tx_volume_df = spark.sql(\"\"\"\n",
    "SELECT from_address as address, SUM(volume) as volume FROM pre_tx_df\n",
    "GROUP BY from_address\n",
    "UNION ALL\n",
    "SELECT to_address as address, SUM(volume) as volume FROM pre_tx_df\n",
    "GROUP BY to_address\n",
    "UNION ALL\n",
    "SELECT token_contract as address, SUM(volume) as volume FROM pre_tx_df\n",
    "GROUP BY token_contract\n",
    "\"\"\").withColumn('volume', format_number('volume', 10)) \n",
    "\n",
    "\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952565b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f58a4968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.7644712924957275"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "tx_volume_df.repartition(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\",True) \\\n",
    "    .csv(\n",
    "        \"{base_path}/tx_volumes/{time_range}/start={start}_end={end}/\" \\\n",
    "            .format(\n",
    "                base_path = base_path, \\\n",
    "                time_range = time_range, \\\n",
    "                start = start, \\\n",
    "                end = end \\\n",
    "            ) \\\n",
    "    )\n",
    "\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4e60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7503624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a2e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
